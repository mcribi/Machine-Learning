{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Asignatura**: Aprendizaje Automático\n",
        "\n",
        "**Proyecto Práctico Opcional**: Aprendizaje por Refuerzo\n",
        "\n",
        "**Valoración máxima**: 10 puntos\n",
        "\n",
        "**Fecha límite de entrega**: 30 de Mayo de 2025 a las 23:59\n",
        "\n",
        "**Procedimiento de entrega**: a través de PRADO\n",
        "\n",
        "\n",
        "<font color=\"blue\">**Trabajo grupal (máximo 2 componentes)**</font>\n",
        "\n",
        "\n",
        "### Nombre completo 1: <mark>POR FAVOR, ESCRIBA AQUÍ SU NOMBRE</mark>\n",
        "### Nombre completo 2: <mark>POR FAVOR, ESCRIBA AQUÍ SU NOMBRE</mark>\n",
        "\n"
      ],
      "metadata": {
        "id": "CnI_EsWXnJqD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Normas de desarrollo y entrega de trabajos**\n",
        "\n",
        "- Única y exclusivamente se debe entregar este Notebook de Colab (fichero `.ipynb`). **No es necesario entregar ninguna memoria externa** (por ejemplo, en `.pdf`).\n",
        "\n",
        "- El código debe estar bien comentado (explicando lo que realizan los distintos apartados y/o bloques), y todas las decisiones tomadas y el trabajo desarrollado (incluyendo los conceptos fundamentales subyacentes) deben documentarse ampliamente en celdas de texto. Es obligatorio documentar las valoraciones y decisiones adoptadas en el desarrollo de cada uno de los apartados. Debe incluirse también tanto una descripción de las principales funciones (Python/scikit-learn) empleadas (para mostrar que el alumno comprende, a nivel técnico, lo que está haciendo), como una valoración razonada sobre la calidad de los resultados obtenidos. **Sin esta documentación, se considera que el trabajo NO ha sido presentado**.\n",
        "\n",
        "- La entrega en PRADO está configurada para permitir sucesivas entregas de la práctica. Desde este punto de vista, se recomienda subir versiones de la práctica a medida que se van realizando los distintos ejercicios propuestos, y no dejarlo todo para el final.  \n",
        "\n",
        "- Se debe respetar la estructura y secciones del Notebook. Esto servirá para agilizar las correcciones, así como para identificar con facilidad qué ejercicio/apartado se está respondiendo.\n",
        "\n",
        "- El código **NO debe escribir nada a disco**.\n",
        "\n",
        "- El **path de lectura desde Google Drive debe ser siempre el mismo**, que es el que se indica en este Notebook.\n",
        "\n",
        "- Una entrega es apta para ser corregida si se puede ejecutar de principio a fin sin errores. Es decir, un ejercicio con errores de ejecución tendrá una calificación de 0.\n",
        "\n",
        "- No es válido usar opciones en las entradas (es decir, utilizar el comando `input()`, por ejemplo, para que el usuario escoja el valor de las variables para ejecutar el programa). Para ello, se deben fijar al comienzo los valores\n",
        "por defecto que se consideren óptimos o que se soliciten en el enunciado.\n",
        "\n",
        "- Se entrega solamente este Notebook, y no los datos empleados.\n",
        "\n",
        "\n",
        "<font color=\"red\"><b>SOBRE TODO, PENSAD EN SI ALGUIEN QUE NO HUBIERA VISTO VUESTRO TRABAJO SERÍA CAPAZ DE ENTENDER LO QUE HABÉIS ESCRITO Y HECHO, Y SI SERÍA CAPAZ DE VALORAR SU CONTENIDO</b></font>.\n",
        "\n",
        "\n",
        "\n",
        "A modo orientativo, las siguientes referencias podrían utilizarse inicialmente para responder a algunas de las cuestiones planteadas a lo largo del cuaderno:\n",
        "\n",
        "- <a href=\"https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf\">https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf</a>\n",
        "\n",
        "- <a href=\"https://aws.amazon.com/es/what-is/reinforcement-learning/\">https://aws.amazon.com/es/what-is/reinforcement-learning/</a>\n",
        "\n",
        "- <a href=\"https://www.datacamp.com/es/tutorial/reinforcement-learning-python-introduction\">https://www.datacamp.com/es/tutorial/reinforcement-learning-python-introduction</a>\n",
        "\n",
        "- <a href=\"https://www.freecodecamp.org/espanol/news/introduccion-a-q-learning-aprendizaje-por-refuerzo/\">https://www.freecodecamp.org/espanol/news/introduccion-a-q-learning-aprendizaje-por-refuerzo/</a>\n",
        "\n",
        "- <a href=\"https://openaccess.uoc.edu/bitstream/10609/151792/1/IntroduccionAlAprendizajePorRefuerzo.pdf\">https://openaccess.uoc.edu/bitstream/10609/151792/1/IntroduccionAlAprendizajePorRefuerzo.pdf</a>\n",
        "\n",
        "- <a href=\"https://www.ibm.com/es-es/topics/reinforcement-learning\">https://www.ibm.com/es-es/topics/reinforcement-learning</a>\n",
        "\n",
        "- <a href=\"https://www.researchgate.net/publication/335112487_Introduccion_al_Aprendizaje_por_Refuerzo\">https://www.researchgate.net/publication/335112487_Introduccion_al_Aprendizaje_por_Refuerzo</a>\n",
        "\n",
        "- <a href=\"https://www.geeksforgeeks.org/reinforce-algorithm/\">https://www.geeksforgeeks.org/reinforce-algorithm/</a>\n",
        "\n",
        "- <a href=\"https://dilithjay.com/blog/reinforce-a-quick-introduction-with-code\">https://dilithjay.com/blog/reinforce-a-quick-introduction-with-code</a>\n"
      ],
      "metadata": {
        "id": "7cUw8yPsnuMq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **¿Qué es el aprendizaje por refuerzo? (2 punto)**"
      ],
      "metadata": {
        "id": "tR9vEYARnkXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Realizad una búsqueda bibliográfica sobre qué es el aprendizaje por refuerzo en *Machine Learning*.\n",
        "\n",
        "A continuación, describid qué es el aprendizaje por refuerzo (usad vuestras propias palabras, **NO ESTÁ PERMITIDO** el uso de IA Generativa para realizar este apartado).\n",
        "\n",
        "En particular, el texto escrito debe contener información que pueda responder a las siguientes preguntas:\n",
        "\n",
        "- ¿Dónde se encuadra el aprendizaje por refuerzo dentro del aprendizaje automático?\n",
        "- ¿Qué dos elementos principales hay involucrados en el aprendizaje por refuerzo?\n",
        "- ¿Cuál es el ciclo de interacción entre el agente y el entorno?\n",
        "- ¿Qué es un agente? ¿Qué es un entorno?\n",
        "- Conceptos descriptivos sobre qué es:\n",
        "   - Un estado y el espacio de estados.\n",
        "   - Una acción y el espacio de acciones.\n",
        "   - Una recompensa.\n",
        "   - Una política. Distinga entre políticas deterministas y no deterministas.\n",
        "   - Una experiencia.\n",
        "   - Un episodio y una trayectoria. Diferencias entre ambos.\n",
        "- ¿Cuál es la estructura subyacente que se supone que tiene un entorno? Ponga un ejemplo de Proceso de Decisión de Markov sencillo con pocos estados y acciones.\n",
        "- ¿Cuál es el objetivo de un agente en el aprendizaje por refuerzo?\n",
        "   - Concepto de retorno (Return).\n",
        "   - Concepto de factor de descuento $\\gamma$, para qué se usa y su influencia en el valor de retorno con valores altos o bajos.\n",
        "- ¿Qué es el valor de un estado $V(s)$?\n",
        "- ¿Qué es el valor de un par estado-acción $Q(s,a)$?\n",
        "\n",
        "\n",
        "Para responder a estas preguntas puede utilizar cualquier fuente bibliográfica, sea un libro, documento o web. Recuerde **citar las fuentes apropiadamente**.\n",
        "\n",
        "\n",
        "En términos generales: <font color=\"red\">se persigue que el estudiante elabore unos <i>mini-apuntes</i> que le permitan estudiar y aprender qué es el aprendizaje por refuerzo, a nivel introductorio.</font>\n"
      ],
      "metadata": {
        "id": "gX_DNsnJd5ff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Aprendizaje por refuerzo clásico (4 puntos)**"
      ],
      "metadata": {
        "id": "1AMGSUW_n86y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## La biblioteca *Gymnasium*\n"
      ],
      "metadata": {
        "id": "b07lnZt7gL6L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Su web principal es <a href=\"https://gymnasium.farama.org/index.html\">https://gymnasium.farama.org/index.html</a>\n",
        "\n",
        "- Busque información sobre la biblioteca Gymnasium e instálela.\n",
        "- Instancie la biblioteca con el entorno ```'CliffWalking-v0'```. Describa, para el entorno generado...:\n",
        "  - Cuál es el espacio de estados.\n",
        "  . Cuál es el espacio de acciones.\n",
        "  - Cuál es el sistema de recompensas.\n",
        "  - Cuál es el sistema de terminación/parada de un episodio.\n",
        "  - Para qué sirve el método **reset()**, qué entradas tiene y qué salidas proporciona y qué información contienen las salidas.\n",
        "  - Para qué sirve el método **step()**, qué entradas tiene y qué salidas proporciona, describiendo el significado de cada una de ellas.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WansH0eGhmiC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implemente de forma manual dos políticas, haciendo uso del **wrapper** *TimeLimit* para limitar el número de experiencias en un episodio a 500:\n",
        "\n",
        "- Una función que implemente una política aleatoria.\n",
        "- Una función que implemente una política óptima.\n",
        "\n",
        "Además:\n",
        "\n",
        "- Ejecute ambas políticas un total de 100 episodios y compare, analice y discuta los resultados obtenidos.\n",
        "- Utilice el parámetro **render** para obtener una visualización de un único episodio con cada política. En caso de obtener algún error de ejecución: <a href=\"https://stackoverflow.com/questions/50107530/how-to-render-openai-gym-in-google-colab\">Stack Overflow</a> y similares.\n"
      ],
      "metadata": {
        "id": "5YfBvCIkho9x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Algoritmos clásicos"
      ],
      "metadata": {
        "id": "u2rOGrdFimox"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Busque información de algoritmos clásicos de Aprendizaje por Refuerzo (Q-Learning, SARSA, Value Iteration, Policy Iteration, ...).\n",
        "- Seleccione uno de ellos.\n",
        "- Describa su funcionamiento a nivel general, pero con rigor (pasos del algoritmo, función de actualización y qué significa cada término, etc.).\n",
        "\n",
        "En términos generales: <font color=\"red\">se debe describir qué métodos existen desde antes de la incursión de Deep Learning para aprendizaje por refuerzo dando una visión general del área, seleccionar uno de los métodos de aprendizaje y explicarlo en detalle.</font>"
      ],
      "metadata": {
        "id": "mPoekH2zinxz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementación"
      ],
      "metadata": {
        "id": "dhJ0Ha5WjhVC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- Implemente desde cero el algoritmo seleccionado.\n",
        "- Describa su funcionamiento (pasos del algoritmo, función de actualización y qué significa cada término, etc.) **y su relación con la descripción general dada en el apartado anterior**.\n",
        "- Aplíquelo sobre algún entorno de la biblioteca Gymnasium, justificando su idoneidad en términos de espacios de estados y acciones, diseño de recompensas, etc.. Justifique también el uso de hiperparámetros (de haberlos). No olvide también describir el entorno seleccionado (espacio de estados, espacio de acciones, recompensas, etc.)\n",
        "- Discuta los resultados obtenidos considerando el número de veces que el entorno ha sido resuelto tras 100 ejecuciones distintas de prueba de la política obtenida con el algoritmo. Si procede, también puede comparar su implementación con la tabla de clasificación dada en <a href=\"https://github.com/openai/gym/wiki/Leaderboard\">https://github.com/openai/gym/wiki/Leaderboard</a>.\n",
        "\n",
        "En términos generales: <font color=\"red\">se debe usar el algoritmo implementado para resolver un problema, y describir los pasos seguidos con detalle enlazando la implementación con la parte teórica.</font>\n",
        "\n"
      ],
      "metadata": {
        "id": "5ckZv-LZjixa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1ZsdOtQIjgHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Aprendizaje por refuerzo profundo (4 puntos)**"
      ],
      "metadata": {
        "id": "DCmF11troDzB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Busque información sobre algoritmos de **Deep Reinforcement Learning** y describa sus fundamentos, en términos globales/generales. En particular, interesa distinguir entre las categorías básicas derivadas de **Deep Q-Networks** y **Policy Gradient**.\n",
        "\n",
        "- Seleccione un algoritmo entre ellos. Se recomienda escoger alguno \"tradicional\" dentro del área de **Deep Reinforcement Learning**.\n",
        "\n",
        "- Describa el funcionamiento general del algoritmo, con sus propias palabras pero con rigor. En particular, interesa la descripción del algoritmo, sus componentes, funcionamiento y entrenamiento de la red de política, etc.. No olvide, especialmente, describir cómo se genera el conjunto de datos de entrada y salida, cuál es la función de pérdida a usar y cómo se efectúa el entrenamiento en cada iteración. **La descripción del algoritmo debe ser general, pero suficientemente detallada como para poder ser implementado tras leer dicha descripción**.\n",
        "\n",
        "En términos generales: <font color=\"red\">se debe describir qué es el aprendizaje por refuerzo profundo dando una visión general del área, seleccionar uno de los métodos de aprendizaje y explicarlo en detalle.</font>"
      ],
      "metadata": {
        "id": "A3Jfq_63jxTc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## La biblioteca de programación"
      ],
      "metadata": {
        "id": "Fb1KIB4aRM3Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Estudie las bibliotecas de programación de aprendizaje por refuerzo <a href=\"https://github.com/tensorflow/agents\">TF-Agents</a> y <a href=\"https://github.com/keiohta/tf2rl\">TF2-RL</a>. Trate de hallar ventajas e inconvenientes de cada una de ellas.\n",
        "\n",
        "- Seleccione alguna de las bibliotecas anteriores que contenga el algoritmo seleccionado en el apartado anterior.\n",
        "\n",
        "- Escoja un entorno de la biblioteca *Gymnasium* (distinto del usado anteriormente), entre los dados en las categorías **Toy Text** o **Classic Control**. No olvide también describir el entorno seleccionado (espacio de estados, espacio de acciones, recompensas, etc.)\n",
        "\n",
        "- Implemente una solución de **Deep Reinforcement Learning** para resolver el problema del entorno seleccionado, utilizando la biblioteca de aprendizaje por refuerzo para Tensorflow escogida.\n",
        "\n",
        "- Describa el uso de la biblioteca de aprendizaje para la resolución del problema (funciones y clases para implementar el algoritmo de aprendizaje y las políticas -y otras redes de ser requeridas por su método escogido-, componentes, parámetros e hiperparámetros...).\n",
        "\n",
        "- Indique cómo se adapta el uso de la biblioteca a la descripción general del algoritmo dada en el apartado anterior (se trata de enlazar la teoría previamente descrita con la práctica en el uso de la biblioteca seleccionada).\n",
        "\n",
        "- Discuta los resultados obtenidos considerando el número de veces que el entorno ha sido resuelto tras 100 ejecuciones distintas de prueba de la política obtenida con el algoritmo. Si procede, también puede comparar su implementación con la tabla de clasificación dada en <a href=\"https://github.com/openai/gym/wiki/Leaderboard\">https://github.com/openai/gym/wiki/Leaderboard</a>.\n",
        "\n",
        "\n",
        "En términos generales: <font color=\"red\">se debe usar la biblioteca seleccionada para resolver un problema con el método de aprendizaje escogido, y describir los pasos seguidos con detalle enlazando la implementación con la parte teórica.</font>\n",
        "\n"
      ],
      "metadata": {
        "id": "zx8IYhmARsf5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4T5H_AvFndyZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}